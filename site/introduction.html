<!DOCTYPE html>
<html>
<head>
<meta http-equiv="Content-type" content="text/html;charset=UTF-8" />
<title>Introduction &middot; Crafting Interpreters</title>

<!-- Tell mobile browsers we're optimized for them and they don't need to crop
     the viewport. -->
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="stylesheet" type="text/css" href="style.css" />
<link href='http://fonts.googleapis.com/css?family=Source+Code+Pro:400|Source+Sans+Pro:300,400,600' rel='stylesheet' type='text/css'>
<link rel="icon" type="image/png" href="image/favicon.png" />
<script src="jquery-1.10.1.min.js"></script>
<script src="script.js"></script>
</head>
<body id="top">

<!-- <div class="scrim"></div> -->
<nav class="wide">
  <a href="/"><img src="image/logotype-small.png" title="Crafting Interpreters"></a>
  <div class="contents">
<!-- If there is a part, it must be a chapter within a part. -->
<h3><a href="#top"><small>1.</small> Introduction</a></h3>

<ul>
    <li><a href="#why-learn-this-stuff">Why learn this stuff?</a></li>
    <li><a href="#how-the-book-is-organized">How the book is organized</a></li>
    <li><a href="#map-of-world-of-languages">Map of world of languages</a></li>
    <li><a href="#how-these-parts-are-organized">How these parts are organized</a></li>
    <li><a href="#our-own-journey">Our own journey</a></li>
    <li><a href="#exercises-(chap-1)">Exercises (chap 1)</a></li>
    <li><a href="#exercises-(chap-2)">Exercises (chap 2)</a></li>
</ul>


<div class="prev-next">
    <a href="welcome.html" title="Welcome">←&nbsp;Previous</a>
    <a href="the-pancake-language.html" title="The Pancake Language" class="right">Next&nbsp;→</a>
</div>  </div>
</nav>

<nav class="narrow">
<a href="/"><img src="image/logotype-small.png" title="Crafting Interpreters"></a>
<a href="welcome.html" title="Welcome" class="prev">←</a>
<a href="the-pancake-language.html" title="The Pancake Language" class="next">→</a>
</nav>

<div class="page">
<div class="nav-wrapper">
<nav class="floating">
  <a href="/"><img src="image/logotype-small.png" title="Crafting Interpreters"></a>
  <div class="expandable">
<!-- If there is a part, it must be a chapter within a part. -->
<h3><a href="#top"><small>1.</small> Introduction</a></h3>

<ul>
    <li><a href="#why-learn-this-stuff">Why learn this stuff?</a></li>
    <li><a href="#how-the-book-is-organized">How the book is organized</a></li>
    <li><a href="#map-of-world-of-languages">Map of world of languages</a></li>
    <li><a href="#how-these-parts-are-organized">How these parts are organized</a></li>
    <li><a href="#our-own-journey">Our own journey</a></li>
    <li><a href="#exercises-(chap-1)">Exercises (chap 1)</a></li>
    <li><a href="#exercises-(chap-2)">Exercises (chap 2)</a></li>
</ul>


<div class="prev-next">
    <a href="welcome.html" title="Welcome">←&nbsp;Previous</a>
    <a href="the-pancake-language.html" title="The Pancake Language" class="right">Next&nbsp;→</a>
</div>  </div>
  <a id="expand-nav">≡</a>
</nav>
</div>

<article class="chapter">

  <div class="number">1</div>
  <h1>Introduction</h1>

<div class="sign-up">
    <h1>This book is a work in progress!</h1>
  <span class="dismiss">&times;</span>
    <p>If you see a mistake, find something unclear, or have a suggestion, please <a href="https://github.com/munificent/crafting-interpreters/issues" target="_blank">file a ticket</a>. To know when new chapters are up, join the mailing list:</p>

  <!-- Begin MailChimp Signup Form -->
  <div id="mc_embed_signup">
  <form action="//gameprogrammingpatterns.us7.list-manage.com/subscribe/post?u=0952ca43ed2536d6717766b88&amp;id=6e96334109" method="post" id="mc-embedded-subscribe-form" name="mc-embedded-subscribe-form" class="validate" target="_blank" novalidate>
    <input type="email" value="" name="EMAIL" class="email" id="mce-EMAIL" placeholder="Your email address" required>
    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->
    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_0952ca43ed2536d6717766b88_6e96334109" tabindex="-1" value=""></div>
    <input type="submit" value="Sign me up!" name="subscribe" id="mc-embedded-subscribe" class="button">
  </form>
  </div>
  <!--End mc_embed_signup-->
  <p class="small">(I post about once a month. Don&#8217;t worry, I won&#8217;t spam you.)</p>
</div>

  <blockquote>
<p>No, no! The adventures first, explanations take such a dreadful time.</p>
<p><cite>Lewis Carroll</cite></p>
</blockquote>
<p>I&rsquo;m really excited we&rsquo;re going on this journey together. This is a book on
implementing interpreters for programming languages. It&rsquo;s also a book on how to
design a language worth implementing. It&rsquo;s the book I wished I had when I first
started getting into languages, and it&rsquo;s the book I&rsquo;ve been writing in my <span
name="head">head</span> for nearly a decade.</p>
<aside name="head">
<p>To my friends and family, sorry I seemed so distracted!</p>
</aside>
<p>In these pages, we will walk step by step through two complete interpreters for
a full-featured language. I assume this is your first foray into languages, so
I&rsquo;ll cover each concept and piece of code you&rsquo;ll need to understand to get to a
complete, usable, fast language implementation.</p>
<p>That&rsquo;s a good bit of distance to cover, so my plan is to take you on a carefully
charted path. To save time, we won&rsquo;t wander far off that path, but there&rsquo;s
plenty of interesting stuff out there. I&rsquo;ll point it out in passing and as you
get more confident, I encourage you to venture out and explore them yourself.</p>
<p>We also won&rsquo;t focus on theory as much as other books do. As we build each piece
of the system, I will introduce the history and concepts behind it. I&rsquo;ll try to
get you familiar with the terminology so that if you ever inadvertantly find
yourself in a cocktail <span name="party">party</span> full of PL (programming
language) researchers, you&rsquo;ll fit right in.</p>
<aside name="party">
<p>Believe it or not, a bizarre situation I have in fact found myself in multiple
times. Some of those people can drink like you wouldn&rsquo;t believe.</p>
</aside>
<p>But we&rsquo;re mostly going to spend our brain juice getting our language up and
running. This is not to say theory isn&rsquo;t important. Being able to reason
precisely and <span name="formal">formally</span> about syntax and semantics is
a vital skill when working on a language. But, personally, I learn best by
doing. It&rsquo;s hard for me to wade through paragraphs full of abstract concepts and
feel I&rsquo;ve really absorbed them. But if I&rsquo;ve implemented a concept, debugged it,
ran it, and poked at it, then I <em>get</em> it.</p>
<aside name="formal">
<p>Static type systems in particular require very strong skill in formal reasoning
and logic. Hacking on a type system has the same feel as proving a theorem in
mathematics. As it turns out, this is no coincidence. In the early half of last
century, Haskell Curry and William Alvin Howard showed that the two are exactly
the same: <a href="https://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence">the Curry-Howard isomorphism</a>.</p>
</aside>
<p>That&rsquo;s my goal for you. I want you to come away with a solid intuition of how a
real language lives and breathes. My hope is this will form the foundation that
you can use to build a more formal understanding on from reading other books.</p>
<h2><a href="#why-learn-this-stuff" name="why-learn-this-stuff">Why learn this stuff?</a></h2>
<p>Every introduction to every language book seems to have this section. I don&rsquo;t
know what it is about programming languages that seems to cause this existential
angst. Perhaps we feel presumptious about creating something that may change the
way others express their ideas. I don&rsquo;t think ornithology books worry about
justifying their existence. They tacitly assume the reader loves birds and move
on.</p>
<p>But programming languages are a little different. I suppose it&rsquo;s true that the
odds of any of us creating a broadly successful general purpose programming
language are slim. The designers of all of the world&rsquo;s successful languages
could fit in a Volkswagen bus, even without putting the pop top camper up. So if
joining that elite group was the only reason to learn about languages, it would
be hard to justify. Fortunately, it isn&rsquo;t.</p>
<h3><a href="#little-languages-are-everywhere" name="little-languages-are-everywhere">Little languages are everywhere</a></h3>
<p>For every successful <em>general purpose</em> language out there, there are a thousand
successful niche ones. We used each one a &ldquo;little language&rdquo;, but today the name
on their trumped-up business card reads &ldquo;domain-specific language&rdquo;. These are
mini-languages, tailor-built to a specific task. Things like
application-specific scripting languages, template engines, markup formats, and
configuration files.</p>
<p>There&rsquo;s a very good chance in your career that you&rsquo;ll find yourself needing one
of these. When you can, it&rsquo;s good to reuse an existing one instead of rolling
your own. Once you take into account the need for documentation, debuggers,
editor support, syntax highlighting, and all of the other tooling, doing it
yourself can be a lot of work.</p>
<p>But there&rsquo;s still a real chance you&rsquo;ll find yourself needing to whip up a parser
or something when there isn&rsquo;t an existing library that fits your needs. Even
when there <em>is</em> one you can reuse, you&rsquo;ll inevitably end up needing to debug and
maintain it and poke around in its guts.</p>
<h3><a href="#languages-are-great-exercise" name="languages-are-great-exercise">Languages are great exercise</a></h3>
<p>When long distance runners train, they sometimes run with weights strapped to
their ankles. This artificial handicap pushes them and when they take off the
weights later, they can run farther and faster.</p>
<p>An efficient language implementation is a real test of programming skill. They
are complex and performance critical. They heavily rely on recursion. They use
data structures like dynamic arrays, trees, graphs, and hash tables.</p>
<p>You probably use those in your day-to-day programming, especially hash tables,
but how well do you <em>really</em> understand them? Well, after we&rsquo;ve crafted our own
from scratch, I can promise you will.</p>
<p>While I hope to teach you that a programming language isn&rsquo;t as daunting as you
might believe, it still a challenge. If you are up to the task, I think you&rsquo;ll
come away a stronger programmer, and smarter about how you use data structures
and algorithms in your day job.</p>
<h3><a href="#one-more-reason" name="one-more-reason">One more reason</a></h3>
<p>This last reason is a little embarrassing to admit. It&rsquo;s sort of the secret goal
of this book. When I first learned to program as a kid, I couldn&rsquo;t conceive of
how a language itself worked. How did they write BASIC before they had BASIC? It
seemed like magic to me.</p>
<p>The problem with magic is that it inherently excludes people. A magician&rsquo;s
illusion only works when the audience doesn&rsquo;t know the secret. The difference
between a wizard and everyone else is that, to the wizard, <em>it&rsquo;s not magic</em>.</p>
<p>If you think of programming languages as a black art known only to certain
arcane practitioners, you are implicitly excluding yourself from that club. My
aim is to show you that there is no magic here.</p>
<p>There are a few techniques you don&rsquo;t often encounter outside of languages, and
some parts are a little difficult. But not more difficult than other engineering
problems you&rsquo;ve tackled. It&rsquo;s the divide between normal programmers and
programming language hackers that is the illusion. Code is just code and people
are just people.</p>
<p>If I can get you to punch through that illusory wall, maybe some of the others
that hold you back will look a little ephemeral too. And, who knows, maybe you
<em>will</em> make the next great language. Someone has to.</p>
<h2><a href="#how-the-book-is-organized" name="how-the-book-is-organized">How the book is organized</a></h2>
<p>This book is broken into three parts. You&rsquo;re reading the first one now. It&rsquo;s a
couple of chapters to get you oriented, teach you some of the lingo language
hackers use, and introduce you to Lox, the language we&rsquo;ll be implementing.</p>
<p>Each of the other two parts builds one complete Lox interpreter. Each
interpreter is built one chapter at a time. It took a good bit of trial and
error on my part, but I managed to carve up each interpreter into chapter-sized
chunks so that each chapter&rsquo;s part builds on the previous ones. At the end of
each chapter, you have an increasingly full-featured interpreter that you can
run an play with.</p>
<h3><a href="#each-chapter" name="each-chapter">Each chapter</a></h3>
<p>Aside from the couple of introductory chapters in this part which are special,
each chapter in the book is structured the same. It takes a single feature
needed in a programming language and teaches you the concepts behind it. At the
same time, it contains every single line of code needed to implement it.</p>
<p>(What this book doesn&rsquo;t contain is the machinery needed to compile and run the
code. I assume you can slap together a makefile or a project in your IDE of
choice in order to get the code to run.)</p>
<p>Many other language books and language implementations use tools like <a href="https://en.wikipedia.org/wiki/Lex_(software)">Lex</a>
and <span name="yacc"><a href="https://en.wikipedia.org/wiki/Yacc">Yacc</a></span>, &ldquo;compiler-compilers&rdquo; to automatically
generate some of the source files for an implementation from some higher level
description.</p>
<aside name="yacc">
<p>Yacc is a tool that takes in a grammar file and produces a source file for a
compiler, so it&rsquo;s sort of like a &ldquo;compiler&rdquo; that outputs a compiler. Hence the
name, &ldquo;compiler-compiler&rdquo;.</p>
<p>It wasn&rsquo;t the first of its ilk, which is why it&rsquo;s named &ldquo;Yacc&rdquo;—Yet Another
Compiler-Compiler. A later similar tool is <a href="https://en.wikipedia.org/wiki/GNU_bison">Bison</a>, named as a pun on the
pronunciation of Yacc like &ldquo;yak&rdquo;.</p>
<p>If you find all of these little self-references and puns charming and fun,
you&rsquo;ll fit right here. If not, well, maybe the language nerd sense of humor will
be an acquired taste for you.</p>
</aside>
<p>There are pros and cons to tools like that, and strong opinions on both sides.
Here, I&rsquo;ve chosen to eschew them. I want to ensure there are no dark corners
where magic and confusion can hide, so we&rsquo;ll be writing everything by hand from
scratch. As you&rsquo;ll see, it&rsquo;s not as bad as it sounds and it means you really
will understand each line of code and how both interpreters work.</p>
<p>A book has different constraints from &ldquo;real world&rdquo; code and so the style of the
code here might not always reflect the best way to write maintainable code in a
normal codebase. I tend to not worry about about access control modifiers like
<code>public</code> and <code>private</code> and don&rsquo;t do things like hide fields behind getters and
setters. The pages here aren&rsquo;t as wide as your IDE and every character counts
when I&rsquo;m trying to make this easy for you to read.</p>
<p>Also, the code here doesn&rsquo;t have many comments. That&rsquo;s because each handful of
lines is surrounded by several paragraphs of honest-to-God prose explaining it.
If you write a book to accompany each of your programs, you can ditch the
comments too! Otherwise, you should probably use <code>//</code> a little more than I do.</p>
<p>Each chapter has a few other <span name="aside">accoutrements</span>.</p>
<aside name="aside">
<p>Asides like this one contain historical notes, references to related topics, and
suggestions of other areas to explore. Well, some do, at least. Most of them are
just dumb jokes and goofy illustrations. Sorry.</p>
<p>You can skip them if you want. I won&rsquo;t judge you.</p>
</aside>
<h3><a href="#exercises" name="exercises">Exercises</a></h3>
<p>The exercises at the end are to help you learn more. Instead of reviewing what
the chapter already told you, they specifically force you to step off the guided
path and explore on your own. They will make you research other languages,
figure out how to implement other language features or otherwise get you to
strike out on your own initiative.</p>
<p>Boldly attack them and you&rsquo;ll come away with a broader understanding and
possibly a few bumps and scrapes. Or skip them if you just want to stay inside
the comfy confines of the tour bus. It&rsquo;s your book.</p>
<h3><a href="#design-notes" name="design-notes">Design notes</a></h3>
<p>Most other books assume the language you&rsquo;re implementing is a given and are
focused on how to implement it. This book mainly does that too. I already did
the work to design Lox so you don&rsquo;t have to.</p>
<p>Focusing on implementation is fun because it is so <span
name="benchmark">well-defined</span>. In the ideal, you have a language spec
already and you just need to crank out some code that implements those semantics
and passes the test suite.</p>
<aside name="benchmark">
<p>I know a lot of language hackers who live entirely within this world. They are
like athletes where their language&rsquo;s benchmark suite determines how well they
score and the only criteria by which they are evaluated.</p>
</aside>
<p>But the softer side of languages, how a human actually uses it effectively, is a
vital part of making your <em>new</em> language successful. Most books don&rsquo;t talk much
about that, I think in large because it is fuzzier. Many computer scientistics
feel uncomfortable talking about anything that can&rsquo;t be proven like a theorem.</p>
<p>I look at languages as, in large part, a user interface. Each is a tool you use
to communicate how a machine should behave to the computer and to the other
programmers maintaining the code. You can&rsquo;t design a good language if you don&rsquo;t
think about the humans using it, even though we Homo sapiens don&rsquo;t have the
pleasant crispness of discrete mathematics.</p>
<p>To touch on that, many chapters also contain a section of &ldquo;design notes&rdquo;. These
are little essays on some corner of the human aspect of programming languages.
What makes features easier or harder to learn, how to grow an ecosystem,
crafting a readable syntax, stuff like that.</p>
<p>I don&rsquo;t claim to be an expert on any of this—I don&rsquo;t know if anyone really
can—so take these with a large pinch of salt. That should make them tastier food
for thought, which is my main aim. If you come away disagreeing with me on all
accounts, but still <em>caring</em> about the user side of languages, that&rsquo;s enough for
me.</p>
<p>On the other hand, if you just want to pump out some code for an interpreter and
get it running, feel free to skip these sections.</p>
<h3><a href="#part-ii-the-first-interpreter" name="part-ii-the-first-interpreter">Part II, The first interpreter</a></h3>
<p><strong>TODO: Condense.</strong></p>
<p>We&rsquo;ll write our first interpreter in Java. It&rsquo;s a good language for teaching
concepts. It has a nice set of collection types and frees us from having to
manage memory. At the same time, it&rsquo;s pretty explicit. Unlike scripting
languages, there tends to be less &ldquo;magic&rdquo; under the hood, and you&rsquo;ve got static
types to see exactly what kinds of objects you&rsquo;re working with.</p>
<p>I also chose it specifically because it is an object-oriented language. That
paradigm swept the programming world in the 90s and is now the dominant way of
thinking for millions of programmers. Odds are good you&rsquo;re already used to
organizing things into classes and methods, so we&rsquo;ll keep you in that comfort
zone.</p>
<p>While the academic language community sometimes shies away from object-oriented
programming, the reality is that it is widely used for language work today as
well. GCC and LLVM are written in C++, as are most JavaScript virtual machines.
Object oriented languages are ubiquitous and the tools and compilers for those
languages are often written in the <span name="host">same language</span>.</p>
<aside name="host">
<p>A compiler is a program that reads in files in one language and translates them
to files in another language. You can implement one in any language, including
the same language it uses for its input. If your compiler is powerful enough, it
can even take in its own source code as input and compile itself. That&rsquo;s called
<strong>&ldquo;self-hosting&rdquo;.</strong></p>
<p>Of course, you need to be able to compile your compiler using some other
compiler you have laying around before you can run it and pass it its own source
code. But once you&rsquo;ve done that once, you now have a compiled version of your
compiler that was produced by your own compiler. Now you can throw away the
version you compiled with the other compiler.</p>
<p>Henceforth, you can keep using previous versions of your own compiler to compile
the next version of it. This is called <strong>&ldquo;bootstrapping&rdquo;</strong> from the image of
pulling yourself up by your own bootstraps. (This is also where we get the term
<strong>&ldquo;booting&rdquo;</strong> for starting up a computer.)</p>
<p><strong>TODO: Illustration.</strong></p>
<p>A language ecosystem needs lots of different tools beyond just the core
compiler. You&rsquo;ll need editors, debuggers, formatters, etc. Most language
designers prefer to write those tools in their own language so they can get some
first-hand experience with their language, and so they aren&rsquo;t as dependent on
other languages. They call this &ldquo;eating your own dogfood&rdquo; or just
<strong>&ldquo;dogfooding&rdquo;</strong>.</p>
</aside>
<p>And, finally, Java is hugely popular. That means there&rsquo;s a good chance you
already know it, so there&rsquo;s less for you to learn to get going in the book. If
you aren&rsquo;t that familiar with Java, don&rsquo;t freak out. I try to stick to a fairly
minimal subset of it. I use the diamond operator from Java 8 to makes things a
little more terse, but that&rsquo;s about it as far as advanced features go. If you
know another object-oriented language like C# or C++, you can probably muddle
through fine.</p>
<p>For our first interpreter, we&rsquo;ll focus mostly on <em>concepts</em>. We&rsquo;ll write the
simplest, cleanest code we can to correctly implement the semantics of the
language. This will get us comfortable with the basic techniques and also hone
our understanding of exactly how the language is supposed to behave.</p>
<p>The end result is a simple, readable implementation, but not a very <em>fast</em> one.
It also leans on Java for managing memory and representing objects. But we want
to learn how the Java virtual machine itself implements those things.</p>
<h3><a href="#part-iii-the-second-interpreter" name="part-iii-the-second-interpreter">Part III, The second interpreter</a></h3>
<p><strong>TODO: Condense.</strong></p>
<p>So in the next part, we&rsquo;ll start all over again, but this time in C. C is the
perfect language for understanding how an implementation <em>really</em> works, all the
way down to the bytes in memory and the code flowing through the CPU. It makes
explicit the few things Java doesn&rsquo;t: how memory is managed, and how objects are
represented.</p>
<p>A big reason that we&rsquo;re using C is so I can show you things C is particularly
good at, but that does mean you&rsquo;ll need to be pretty familiar with it. You don&rsquo;t
have to be the reincarnation of Dennis Ritchie, but you shouldn&rsquo;t be spooked by
pointers either.</p>
<p>If you aren&rsquo;t there yet, pick up an introductory book on C and chew through it,
then come back here when you&rsquo;re done. In return, you&rsquo;ll come away from this book
an even stronger C programmer.</p>
<p>That alone is a useful skill. C is still widely used for a variety of domains,
and many language implementations use it, especially scripting languages. Lua,
CPython, and Ruby&rsquo;s MRI are all written in C.</p>
<p>Our C interpreter forces us to implement ourselves all the things Java gave us
for free. We&rsquo;ll write our own dynamic array and hash table. We&rsquo;ll decide how
objects are represented in memory, and build a garbage collector to manage it.</p>
<p>Our Java implementation was focused on being correct. Now that we have that
down, we&rsquo;ll turn to also being <em>fast</em>. Our C interpreter will contain a compiler
that translates the code to an efficient bytecode representation which it then
executes. This is the same technique used by <span name="impl">implementations
of</span> Lua, Python, Ruby, PHP and many other successful languages.</p>
<aside name="impl">
<p>The &ldquo;implementations of&rdquo; part is significant in this sentence. There are
implementations of Python that use bytecode, and others that compile to native
code. That means words like &ldquo;compiled&rdquo; or &ldquo;interpreted&rdquo; don&rsquo;t describe a
<em>language</em>, just one particular language <em>implementation</em>.</p>
</aside>
<p>We&rsquo;ll even do a little benchmarking and optimization. By the end we&rsquo;ll have a
robust, accurate, fast interpreter for our language, able to keep up with other
professional caliber language implementations out there.</p>
<h2><a href="#map-of-world-of-languages" name="map-of-world-of-languages">Map of world of languages</a></h2>
<p>This book isn&rsquo;t a broad survey and we aren&rsquo;t going to wander all over the place,
but it&rsquo;s worth at least looking at a map of the territory covered by languages.
It will help us understand where we are going and alternate paths other language
implementations take.</p>
<p>This is also a good time to get some basic terminology down. Have you ever
wondered what the difference between a &ldquo;compiler&rdquo; and an &ldquo;interpreter&rdquo; is? Now
you&rsquo;ll find out.</p>
<p>Unfortunately, questions like that are surprisingly fuzzy in computer science, a
field that claims to prize precision. Many of these terms were coined in a time
when computers ran literally a hundred thousand times slower and didn&rsquo;t have
enough storage to hold an entire source file in memory. The world has moved on,
but the terms haven&rsquo;t so we keep stretching their definitions to try to match
today&rsquo;s usage.</p>
<p>This is why even a seemingly simple questions like &ldquo;Is language X compiled or
interpreted?&rdquo; or even &ldquo;What&rsquo;s the difference between a compiler and an
interpreter?&rdquo; get so nebulous.</p>
<p>Fortunately, though, almost all language implementations over the entire history
of languages follow a couple of well-worn paths. Some may skip a step or two,
but you&rsquo;ll find a surprising amount of similarity in a Rear Admiral Grace
Hopper&rsquo;s first COBOL compiler and some brand-new not-even-beta-yet language
implementation with all of two commits on GitHub today.</p>
<p>Language implementations work in a series of phases. I think of them as climbing
over a mountain. It starts off at the bottom with the program as base source
text, literally just a string of <span name="chars">characters</span>.</p>
<aside name="chars">
<p>Already the language designer has to start making some decisions around the
flexibility and the usability of their language. Are source programs ASCII?
Unicode? What encoding? Do you allow non-ASCII characters in comments? Strings?
Variable names?</p>
</aside>
<p>Each phase analyzes the program and transforms it to some higher-level
representation where the semantics—what the author wants the computer to
do—becomes more obvious. Eventually we reach the peak, the point where our
implementation understands all it needs to about the user&rsquo;s program and knows
what it needs to do to be able to execute it.</p>
<p>Now we start going back down the other side of the mountain. We transform from
this highest-level representation back down to successive lower-level forms to
get closer and closer to something we know how to make the CPU actually execute.</p>
<p>There are a few branches along this path, and a couple of different endpoints on
the other side of the mountain, but this structure is remarkably similar across
almost all language implementations.</p>
<p>Here&rsquo;s the whole picture:</p>
<p><strong>TODO: Alt text.</strong></p>
<p><img src="image/introduction/mountain.png" alt="???" class="wide" /></p>
<p>Now let&rsquo;s go through each of those trails and stopping points. Our journey
begins on the left with the source code the user has written in our language.</p>
<h3><a href="#scanning" name="scanning">Scanning</a></h3>
<p>The first step is <strong>scanning</strong>, also known as <strong>lexing</strong> or (if you want to
sound fancy) <strong>lexical analysis</strong>. They all mean the pretty much same thing. I
like &ldquo;lexing&rdquo; because it sounds like something an evil supervillain would do,
but I&rsquo;ll use &ldquo;scanning&rdquo; here because it seems to be marginally more common in
usage.</p>
<p>A <strong>scanner</strong> takes in the linear stream of <em>characters</em> and chunk them together
into a series of something more akin to &ldquo;words&rdquo;. &ldquo;Lexical&rdquo; comes from the Greek
root &ldquo;lex&rdquo;, meaning &ldquo;word&rdquo;.</p>
<p>In programming languages, each of these words is called a <strong>token</strong>. Some tokens
are single characters, like <code>(</code> and <code>,</code>. Others may be several characters long,
like numbers (<code>123</code>), string literals (<code>"hi!"</code>), and identifiers (<code>name</code>).</p>
<p>Some characters in a source file don&rsquo;t actually mean anything. Whitespace is
often insignificant and comments, by definition, are ignored by the language.
Scanning is the step where these usually get discarded.</p>
<p><strong>TODO: pipeline picture</strong></p>
<h3><a href="#parsing" name="parsing">Parsing</a></h3>
<p>The next step is <strong>parsing</strong>. This is where our syntax gets a <strong>grammar</strong>—the
ability to compose larger phrases, expressions, and statements out of smaller
parts. Did you ever diagram sentences in English class? If so, you&rsquo;ve basically
already done this. Except that English has thousands and thousands of &ldquo;keywords&rdquo;
and imperial tonnes of ambiguity. Programming languages are much simpler.</p>
<p>A <strong>parser</strong> takes a series of tokens and builds a tree structure that
explicitly encodes the nested nature of the grammar. These trees have a couple
of different names—<strong>parse tree</strong> or <strong>abstract syntax tree</strong>—depending on how
close to the grammatical structure to the language they are. In practice, most
language hackers just call them <strong>&ldquo;syntax trees&rdquo;</strong>, <strong>&ldquo;ASTs&rdquo;</strong> or often just
<strong>&ldquo;trees&rdquo;</strong>.</p>
<p>During parsing, we handle things like operator <span
name="precedence">precedence</span> so in <code>a + b * c</code>, the parser&rsquo;s job is to
build a tree that reflects that <code>b * c</code> is evaluated before adding it to <code>a</code>.
Parsing is also where we can detect and report most <strong>syntax errors</strong> like in <code>a
+ * b</code>.</p>
<aside name="precedence">
<p>That is assuming that your language <em>has</em> operator precedence. Some languages
like Smalltalk don&rsquo;t.</p>
</aside>
<p>Parsing has a long, rich history in computer science that is closely tied to the
artifical intelligence community. Many of the techniques used today to parse
programming languages were originally conceived to parse <em>human</em> languages by AI
researchers who were trying to get computers to talk to us.</p>
<p>It turns out human languages are often too ambiguous for the rigid grammars
those parsers could handle, but they were a perfect fit for the simpler
artificial grammars of programming languages.</p>
<p><strong>TODO: tokens to tree picture</strong></p>
<p>Some programming languages begin interpreting code right after they parse it.
Instead of doing any other ahead-of-time analysis or optimization, they simply
take the syntax tree and start interpreting it one branch and leaf at a time.
Any additional work they need to do to understand the user&rsquo;s code will be done
on the fly as each node is processed.</p>
<p>This style of interpretation is common for student projects and really simple
languages, but not widely used for general-purpose languages since it tends to
be slow. A notable exception is the original implementation of <span
name="ruby">Ruby</span> which worked this way before version 1.9.</p>
<aside name="ruby">
<p>At 1.9, the canonical implementation of Ruby switched from the original MRI
(&ldquo;Matz&rsquo; Ruby Interpreter&rdquo;) to Koichi Sasada&rsquo;s YARV (&ldquo;Yet Another Ruby VM&rdquo;). YARV
is a bytecode virtual machine, another style of intepreter that we&rsquo;ll get to in
a bit.</p>
</aside>
<p>Some people use &ldquo;interpreter&rdquo; to mean these kinds implementations, but that can
be vague, so I&rsquo;ll use the sometimes-heard and more explicit <strong>&ldquo;tree-walk
interpreter&rdquo;</strong> to refer to these. Our first interpreter will roll this way.</p>
<h3><a href="#static-analysis" name="static-analysis">Static analysis</a></h3>
<p>The first two stages are pretty similar among almost all implementations and
forms the <strong>front end</strong> of the pipeline. Now, the individual characteristics of
each language start coming into play. At this point, we know the grammatical
structure of the code, operator precedence, when an identifier is declaring a
variable versus accessing, etc. but we don&rsquo;t know much more than that.</p>
<p>For example, in an expression like <code>a + b</code>, we don&rsquo;t know what <code>a</code> and <code>b</code> refer
to. Are they local variables? Global? Where are they defined?</p>
<p>The first bit of analysis that most languages do is called <strong>binding</strong> or
<strong>resolution</strong>. For each <strong>identifier</strong> (variable name) we find out where that
name is defined and wire the two together. This is where <strong>scope</strong> comes into
play—the region of source code where a certain name can be used to refer to a
certain declaration.</p>
<p>If the language is <span name="type">statically typed</span>, this is when we
type check. Once we know where <code>a</code> and <code>b</code> are declared, we can also figure out
their types. Then if those types don&rsquo;t support being added to each other, we
report a <strong>type error</strong>.</p>
<aside name="type">
<p>The language we&rsquo;ll build in this book is dynamically typed, so it will do its
type checking later, at runtime.</p>
</aside>
<p>Now we have summitted the peak of the mountain. We have figured out everything
we can about the user&rsquo;s program without actually running it. All this data that
we&rsquo;ve figured out through analysis needs to be stored somewhere. There are a
variety of places we can squirrel it away. Often, it gets stored right back as
<strong>attributes</strong> on the syntax tree itself—little extra fields that aren&rsquo;t
populated during parsing but get filled in later.</p>
<p>Other times, we may store it in a look-up table stored off to the side. For
example, we may use one to store the types of expressions. Give it an
expression, and it will return the previously calculated static type of it. More
often, the keys to this table are identifiers—names of variables and
declarations. In that case, we call it a <strong>symbol table</strong> and the values it maps
to names will help us understand what that name refers to.</p>
<h3><a href="#optimization" name="optimization">Optimization</a></h3>
<p>We&rsquo;ve done all the static analysis we can and reported errors for things that
didn&rsquo;t add up. We know the code is <span name="correct">correct</span>, and we
know what it means. What else can we do with this knowledge?</p>
<aside name="correct">
<p>&ldquo;Correct&rdquo; at least as far as the language is concerned. It&rsquo;s a valid program, it
might just be a valid program that doesn&rsquo;t do what the programmer wants it to
do.</p>
</aside>
<p>You&rsquo;ve already read the subheader so you know the answer: we can optimize the
code. Since we know the semantics of the user&rsquo;s program, we are now free to
transform it into a different program that has the same semantics but implements
them more efficiently.</p>
<p>A simple example is <strong>constant folding</strong>: if some expression always evaluates to
the exact same value, we can do the evaluation now and replace the code for the
expression with its result. If the user typed in:</p>
<div class="codehilite"><pre><span></span><span class="n">pennyArea</span> <span class="o">=</span> <span class="mf">3.15159</span> <span class="o">*</span> <span class="o">(</span><span class="mf">0.75</span> <span class="o">/</span> <span class="mi">2</span><span class="o">)</span> <span class="o">*</span> <span class="o">(</span><span class="mf">0.75</span> <span class="o">/</span> <span class="mi">2</span><span class="o">);</span>
</pre></div>


<p>We can do all of that arithmetic in the compiler and change their code to:</p>
<div class="codehilite"><pre><span></span><span class="n">pennyArea</span> <span class="o">=</span> <span class="mf">0.44319234375</span><span class="o">;</span>
</pre></div>


<p>To enable optimizations like this, we often transform the syntax tree into some
other <strong>intermediate representation</strong> (or <strong>&ldquo;IR&rdquo;</strong>). Each is a data structure
designed to make later optimizations and phases of the compiler easier to
implement. You can think of it as a pipeline where each stage&rsquo;s job is to
reorganize the code in a way to make the next stage simpler.</p>
<p>One common first step is eliminating <span name="sugar"><strong>syntactic
sugar</strong></span>. Syntactic sugar is a dusting of grammatical niceties that make
your code more pleasant to read and write, but that don&rsquo;t let you express
anything you couldn&rsquo;t write down using other existing, more tedious language
features.</p>
<aside name="sugar">
<p>This delighful turn of phrase was coined by Peter J. Landin in 1964 to describe
how some of the nice expression syntaxes supported by languages like ALGOL were
a sugaring over the more fundamental, yet presumably less palatable lambda
calculus underneath.</p>
<p><strong>TODO: picture?</strong></p>
</aside>
<p>For example, in C, loops are mere syntactic sugar for labels and jumps. A bit of
code like:</p>
<div class="codehilite"><pre><span></span><span class="k">for</span> <span class="p">(</span><span class="n">a</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">a</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">;</span> <span class="n">a</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">&quot;%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>


<p>Can be <strong>desugared</strong> to:</p>
<div class="codehilite"><pre><span></span>  <span class="n">a</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="nl">for_start</span><span class="p">:</span>
  <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="p">(</span><span class="n">a</span> <span class="o">&lt;</span> <span class="mi">10</span><span class="p">))</span> <span class="k">goto</span> <span class="n">for_end</span><span class="p">;</span>
  <span class="n">a</span><span class="o">++</span><span class="p">;</span>
  <span class="n">printf</span><span class="p">(</span><span class="s">&quot;%d</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">a</span><span class="p">);</span>
  <span class="k">goto</span> <span class="n">for_start</span><span class="p">;</span>
<span class="nl">for_end</span><span class="p">:</span>
</pre></div>


<p>And, of course, even that little <code>++</code> can be desugared to a more explicit
assignment. Expanding these shorthands to more verbose but less numerous
constructs makes later passes easier, since they have fewer different language
features that handle.</p>
<p>There are a handful of well-established intermediate representations and a
veritable bestiary of carefully named and studied optimizations that can be
applied to code once translated to one of them.</p>
<p>Optimization is a huge part of language academia as well as industry. Many
language hackers spend their entire careers here, squeezing every drop of
performance they can out of their compilers to get their benchmarks a fraction
of a percent faster. It can become a sort of obsession.</p>
<p>We&rsquo;re going to shy away from that. Many successful languages have surprisingly
few compile-time optimizations. Lua and CPython have glide down the back side of
the mountain with few stops along the way, and focus most of their optimization
effort on the runtime.</p>
<h3><a href="#code-generation" name="code-generation">Code generation</a></h3>
<p>OK, we have applied all of the optimizations to the code we can think of. It&rsquo;s
correct, efficient. The last step is converting it to a form the machine can
actually execute. In other words <strong>generating code</strong>, where &ldquo;code&rdquo; refers to the
kind of primitive assembly-like instructions a CPU runs and not the kind of
&ldquo;source code&rdquo; a human might want to read.</p>
<p>Now we&rsquo;re going down the mountain in the <span name="back"><strong>back end</strong></span>.
From here on out, our representation of the code will become increasingly more
primitive as we get closer and closer to a form the underlying machine can
actually execute. Where the front end takes code from the characters the
programmer wrote to the semantics they intended, the back end takes it from the
high level semantics to the low level steps a computer can follow.</p>
<aside name="back">
<p>Historically, compilers used to do little optimization and most analysis was
done directly inside the parsing code leading to the &ldquo;front&rdquo; and &ldquo;back&rdquo; halves.
As compilers got more sophisticated, more and more work was happening between
those two phases that didn&rsquo;t clearly belong to either side.</p>
<p>To be consistent with &ldquo;front end&rdquo; and &ldquo;back end&rdquo;, they gave it the charming but
spatially confusing name <strong>&ldquo;middle end&rdquo;</strong>.</p>
</aside>
<p>We have a decision to make here. Do we generate instructions for a real CPU or a
virtual one? In what we usually think of as a &ldquo;compiler&rdquo;, the answer is to
generate native machine code. This is the language your CPU itself executes, so
with that, the user&rsquo;s program is in a form that can be directly run by just
throwing the generated machine code at the chip. (That&rsquo;s why they call this form
an &ldquo;executable&rdquo;.)</p>
<p>Targetting native code tends to be the fastest way to implement a language, but
is also a lot of work. Today&rsquo;s CPU architectures have piles of instructions,
complex pipelines, and enough <span name="aad">historical
baggage</span> to fill a 747.</p>
<aside name="aad">
<p>For example, the <a href="http://www.felixcloutier.com/x86/AAD.html">AAD</a> (&ldquo;ASCII Adjust AX Before Division&rdquo;) instruction lets
you perform division, which sounds useful. Except that instruction works on two
binary-coded decimal digits that are packed in a single 16-bit register. When
was the last time you used BCD on a 16-bit machine?</p>
</aside>
<p>Speaking the chip&rsquo;s language also means your compiler is tied to a specific
architecture. If your compiler targets <a href="https://en.wikipedia.org/wiki/X86">x86</a> machine code, it&rsquo;s not going to
run on an <a href="https://en.wikipedia.org/wiki/ARM_architecture">ARM</a> device. All the way back in the 60s, during the Cambrian
explosion of computer architectures, that lack of portability was a real
obstacle.</p>
<p>To get around that, hackers like Martin Richards and Niklaus Wirth, of BCPL and
Pascal fame, respectively, made their compilers generate <em>virtual</em> machine code.
Instead of instructions for some real chip, they&rsquo;d produce code for a
hypothetical, idealized machine. Wirth called this <strong>&ldquo;p-code&rdquo;</strong> for &ldquo;portable&rdquo;,
but today, we generally call it <strong>bytecode</strong> because each instruction is often a
single byte long.</p>
<p>Since the actual chip you&rsquo;re running on doesn&rsquo;t speak that bytecode language,
you still have some work to do. Again, you have two options. You can write a
little mini-compiler for each target architecture that translates the bytecode
to native code for that machine. You still have to do work for each chip you
support, but this last stage is pretty simple and you get to reuse your front
end and code generator across all of the machines you support.</p>
<p>Or you can write a <strong>virtual machine</strong> (<strong>&ldquo;VM&rdquo;</strong>), a program that emulates the
operation of the hypothetical chip that supports your virtual instructions.
Running bytecode on top of a VM is a little slower than running native code
directly because you&rsquo;ve got a layer of abstraction and simulation in there, but
you get a lot of simplicity and portability in return. If the VM itself is
implemented in a portable language like C, then <em>your</em> language now supports any
architecture you can compile C to.</p>
<h3><a href="#runtime" name="runtime">Runtime</a></h3>
<p>We have finally turned the user&rsquo;s program into a form that we can execute. The
last step is actually running it. If we compiled it to machine code, we can just
tell the operating system to load the executable and off it goes. If we compiled
it to bytecode, we need to start up the VM and load the program into that.</p>
<p>In both cases, for all but the most lowest-level of languages, we will usually
need to provide some services that our language declares are available while the
program is running.</p>
<p>For example, if the language is defined to automatically manage memory, we&rsquo;ll
need something like a garbage collector going while the program runs in order to
reclaim unused memory. If our language supports dynamic &ldquo;instance of&rdquo; tests so
you can see why type an object has, then we need some representation to keep
track of the type of each object during execution.</p>
<p>All of this stuff is going at runtime, so it&rsquo;s called, well, the <strong>&ldquo;runtime&rdquo;</strong>.
In a fully compiled language, the code implementing the runtime services gets
inserted directly into the compiled executable. In, say, <a href="https://golang.org/cmd/go/">Go</a>, each compiled
application has its own copy of Go&rsquo;s runtime directly embedded in it.</p>
<p>If the language is run inside an interpreter or VM, that host executable
contains the runtime. This is how most implementations of languages like Java,
Python, and JavaScript work.</p>
<h3><a href="#transpilers" name="transpilers">Transpilers</a></h3>
<p>Most languages that &ldquo;compile&rdquo;—transform the user&rsquo;s code—translate it into
another language at a lower level of abstraction than the input. Typically,
you&rsquo;re taking some high-level human-friendly language and lowering it to machine
code or bytecode.</p>
<p>But, instead of traipsing <em>down</em> the mountain, you could take a sort of sideways
shortcut by translating the input language to another language that&rsquo;s about as
high level. Then you use the existing tools for <em>that</em> language to descend the
mountain to something you can run.</p>
<p>A program that did this used to be called a <strong>&ldquo;source-to-source compiler&rdquo;</strong> or
<span name="transformer">a</span> <strong>&ldquo;transcompiler&rdquo;</strong>. Ever since the rise of
languages that compile to JavaScript in order to run in the browser, they&rsquo;ve
gotten the shorter label <strong>&ldquo;transpiler&rdquo;</strong>.</p>
<aside name="transformer">
<p>&ldquo;Transcompiler&rdquo; is also the name of the nerdiest Transformer to make its way off
Cybertron.</p>
</aside>
<p>The first transcompiler translated 8088 assembly code into 8086 assembly. These
days, almost all transpilers work on higher-level languages. After the rise of
UNIX, there began a long tradition of compilers that produced C as their output
language. C compilers were widely available for a number of architectures and
produced efficient code, so translating to C was a good way to get your language
running on a lot of machines.</p>
<p>Web browsers are the &ldquo;machines&rdquo; of today, and their &ldquo;machine code&rdquo; is
JavaScript, so these days it seems almost every language out there has a
compiler that targets JS since that&rsquo;s the <span name="js">only</span> way to get
your code running in a browser.</p>
<aside name="js">
<p>JS may not be the only language browsers natively support for much longer. If
<a href="https://github.com/webassembly/">Web Assembly</a> takes off, browsers will support another lower-level language
specifically designed to be targeted by compilers.</p>
</aside>
<p>The front end—scanner and parser—of a transpiler looks like other compilers. If
the source language is just a simple syntactic skin over the target language, it
may skip analysis entirely and go from the syntax tree straight to outputting
the analogous syntax in the destination language.</p>
<p>If the two languages are more semantically different, then you&rsquo;ll see more of
the typical phases of a full compiler including analysis and possibly even
optimization. Then, when it comes to code generation, instead of outputting some
binary language like machine code, you produce a string of grammatically correct
source (well, destination) code in the target language.</p>
<h2><a href="#how-these-parts-are-organized" name="how-these-parts-are-organized">How these parts are organized</a></h2>
<p>Those are all the different phases and parts you&rsquo;re likely to see in any
language implementation. Not every implementation has all of these pieces, but
few implementations have major parts not covered by this list.</p>
<p>Some implementations collapse a few phases together. Languages that use <a href="http://bford.info/packrat/">parsing
expression grammars</a> as their parsing strategy often combine scanning into
it too into a single holistic grammar that goes from the language syntax all the
way down to individual characters.</p>
<p>Many simple compilers <span name="sdt">interleave</span> parsing, analysis, and
code generation so that they can generate output code without creating any
explicit syntax trees or other IRs. These <strong>single-pass compilers</strong> limit the
design of the language. You have no intermediate data structures to store global
information about the program, and you don&rsquo;t revisit any previously parsed part
of the code. That means as soon as you parse some expression, you need to
already know enough to correctly compile it.</p>
<aside name="sdt">
<p><a href="https://en.wikipedia.org/wiki/Syntax-directed_translation">Syntax-directed translation</a> is a structured technique to help build
these all-at-once compilers. You associate an <em>action</em> with each grammar rule,
usually one that generates output code. Then, whenever the parser matches that
piece of the grammar, it executes that action, building up the target code one
rule at a time.</p>
</aside>
<p>Some older languages were designed around these constraints. At the time, memory
was so precious that a compiler might not even be able to hold an entire source
file in RAM, much less the rest of the program. So C and Pascal, among others,
were designed such that it was possible to compile them a tiny piece at a time.
This is why both require explicit <em>forward declarations</em> for any function or
type you want to use that hasn&rsquo;t been defined yet.</p>
<h3><a href="#developers-and-users" name="developers-and-users">Developers and users</a></h3>
<p>Even in implementations that have the exact same phases, there is often an
interesting split point in them. In many languages, the process of going from
the source code the developer authors to a form the user can run happens on the
developer&rsquo;s machine, on the end user&rsquo;s machine, or some mixture of the two.</p>
<p>In a traditional &ldquo;compiled&rdquo; language like C or Go that is compiled all the way
to machine code, the entire pipeline runs on the developer&rsquo;s machine. The end
user gets a binary that they can directly execute.</p>
<p>In scripting languages like JavaScript, Python, Ruby, etc. the program is
distributed to the user as its original source code. All of the stages from
scanning through to code generation happen on the end user&rsquo;s machine every time
they run the program.</p>
<p>Other languages split the difference. On the developer&rsquo;s machine, the code is
compiled to some bytecode, which is what the user receives. Then, on the user&rsquo;s
machine a virtual machine loads that bytecode and interprets it. This is how
Java and C#, and other languages that target the Java Virtual Machine (JVM) or
Common Language Runtime (CLR) are executed.</p>
<p>In practice, advanced interpreters and bytecode VMs often contain within them
one final translation to machine code. The HotSpot JVM, Microsoft&rsquo;s CLR and most
JavaScript interpreters will take the bytecode or analyzed source code and
compile it all the way to optimized machine code at runtime on the end user&rsquo;s
machine.</p>
<p>This process of generating native code right at the last second before it&rsquo;s run
is called <strong>&ldquo;just-in-time compilation&rdquo;</strong>. Most hackers just say &ldquo;JIT&rdquo;,
pronounced like it rhymes with &ldquo;fit&rdquo;. The most hardcore VMs JIT the code
multiple times with greater levels of optimization as they discover which
corners of the user&rsquo;s program are performance <span name="hot">hot spots</span>.</p>
<aside name="hot">
<p>This is, of course, exactly where the HotSpot JVM gets its name.</p>
</aside>
<p><strong>TODO: The above is a lot of prose. Illustration, list, or subheader?</strong></p>
<h3><a href="#compilers-and-interpreters" name="compilers-and-interpreters">Compilers and interpreters</a></h3>
<p>Now we can get back to our original question. Why <em>is</em> the difference between a
compiler and an interpreter? In a lot of ways, this is like asking the
difference between a fruit and a vegetable. That <em>sounds</em> like a single binary
choice, but &ldquo;fruit&rdquo; is a botanical term and &ldquo;vegetable&rdquo; is culinary. One does
not imply the negation of the other. That means there are fruits that aren&rsquo;t
vegetables (apples), vegetables that are not fruits (lettuce), but also things
that are <em>both</em> fruits and vegetables (tomatoes).</p>
<p><strong>Compilation</strong> is an <em>implementation technique</em> where you translate a source
language to some other—usually lower-level—form. When you generate bytecode or
machine code, you are compiling. When you transpile to another high-level
language you are compiling too. If users run a tool that takes a source language
and outputs some target language and then stops, we call that tool a
<strong>compiler</strong>.</p>
<p><strong>Interpretation</strong> describes the <em>user experience of executing a language</em>. If
end users run a program from source without having to first go through a
separate tool to process the code, the thing they use to run their program is an
<strong>interpreter</strong>.</p>
<p>First off, recall that neither compilation nor interpretation is a property of a
<em>language</em>. There are Scheme implementations that compile to C or machine code,
and others that interpret directly from source.</p>
<p>In many language implementations, &ldquo;compile&rdquo; and &ldquo;interpret&rdquo; are indeed disjoint
concepts. GCC and Clang take your C code and compile it to machine code. An end
user runs that executable directly and may never even know which tool was used
to compile it. So those are <em>compilers</em> for C.</p>
<p>In older versions of Matz&rsquo; canonical implementation of Ruby, the user ran Ruby
from source. The implementation parsed it and ran it directly by traversing the
syntax tree. No other translation occurred. So this was definitely an
<em>interpreter</em> for Ruby.</p>
<p>But what of CPython? When you run your Python program using it, it is parsed and
converted to an internal bytecode format, which is then executed inside the VM.
From the user&rsquo;s perspective, this is clearly an interpreter—they run their
program from source. But if you look under CPython&rsquo;s scaly skin, you&rsquo;ll see that
there is clearly some compiling going on.</p>
<p>The answer is that it is <span name="go">both</span>. CPython <em>is</em> an
interpreter, and it <em>has</em> a compiler. In practice, most scripting languages work
this way. Interpreting straight from the parsed syntax tree is usually too slow
to be practical, so almost all widely-used interpreters contain an internal
compiler.</p>
<p>This is how the second interpreter we&rsquo;ll build works. So while this book is
nominally about &ldquo;interpreters&rdquo;, we&rsquo;ll also learn compilation too.</p>
<aside name="go">
<p>The <a href="https://golang.org/cmd/go/">Go tool</a> is even more of a chimera. If you run <code>go build</code>, it compiles
your Go source code to machine code and stops. If you type <code>go run</code>, it does
that then immediately executes the generated executable.</p>
<p>So <code>go</code> <em>has</em> a compiler, <em>is</em> an interpreter and <em>is</em> also a compiler.</p>
</aside>
<p><strong>TODO: illustration showing various lang impls and which phases they have and which run on user&rsquo;s machine</strong></p>
<h2><a href="#our-own-journey" name="our-own-journey">Our own journey</a></h2>
<p>That&rsquo;s a lot to take in all at once. Don&rsquo;t worry. This isn&rsquo;t the chapter where
you&rsquo;re expected to <em>understand</em> all of these pieces and parts. I just want you
to know that they are out there and roughly how they fit together.</p>
<p>This map should serve you well as you explore the territory well beyond the
guided path we take in this book. I want to leave you yearning to strike out on
your own and wander all over that mountain.</p>
<p>But, for now, it&rsquo;s time for our own journey to begin. Tighten your bootlaces,
cinch up your pack, and come along. From here on out, all you need to focus on
is the path in front of you.</p>
<h2><a href="#exercises-(chap-1)" name="exercises-(chap-1)">Exercises (chap 1)</a></h2>
<ol>
<li>There are least six domain-specific languages used in the little system I
   cobbled together for myself to write and publish this book. What are they?</li>
</ol>
<p><strong>TODO: More.</strong></p>
<h2><a href="#exercises-(chap-2)" name="exercises-(chap-2)">Exercises (chap 2)</a></h2>
<ol>
<li>
<p>Pick an open source implementation of a language you like. Download the
   source code and poke around in it. Try to find the code that implements the
   scanner and parser. Are they hand-written, or generated using tools like
   Lex and Yacc? (<code>.l</code> or <code>.y</code> files tend to imply the latter.)</p>
</li>
<li>
<p>Most Lisp implementations that compile to C must also contain an interpreter
   to let them execute Lisp code on the fly as well. Why?</p>
</li>
</ol>
<p><strong>TODO: More.</strong></p>

<footer>&copy; 2016 Robert Nystrom</footer>
</article>

</div>
</body>
</html>